---
title: "Predicting shear modulus at small strain from CPTU data, using ensembled machine learning"
author: "Erdi Myftaraga"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
    number_sections: yes
    code_folding: none
    highlight: zenburn
    theme:
      bootswatch: cosmo
      highlight: tango
      bg: '#F7F7F7FF'
      fg: '#171717FF'
      primary: '#C9DB74FF'
      base_font:
        google: Open Sans
      heading_font:
        google: Fira Code
      code_font:
        google: JetBrains Mono
      font-size-base: 0.95rem
      font-weight-base: 400
      headings-font-weight: 900
      headings-color: '#171717FF'
---

```{=html}
<style type="text/css">
.main-container {
  max-width: 1500px;
  margin-left: auto;
  margin-right: auto;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 8.5, fig.align = "center")
thematic::thematic_rmd() # automatic plots theming
```

***

_HINTS: Click the `Hide Toolbars` button on the bottom-right for a better view and use the `Table of Content` on the top-left to navigate through the document._

***

# INTRODUCTION

The following is a short post on predicting small-strain stiffness properties from CPTU measurements by ensembling several machine learning (ML) models. More specifically, we are going to predict the value of the shear modulus at small strain, `Gmax`. The basic workflow can be summarized in these steps:  

* read, explore and prepare the data;
* train several models using measurements of `Gmax` values derived from SCPT tests;
* combine the output of these models to generate an ensembled model;
* use the final ensembled model to generate `Gmax` values from all other CPTU measurements.  

Here I will use open-source data from the HKN offshore wind farm in the Netherlands, which are provided from the [RVO WEBSITE](https://offshorewind.rvo.nl/) üëèüëèüëè

It is important to emphasize that this is mainly a demonstrative blog post, meaning that I am not doing very technically-strict analyses here. Although I am going to work on various aspects on geotechnical site assessment, those topics won't be treated in detail, and often, simplified approaches or existing analyses will be used. I will try to be careful to emphasize the points where these "shortcuts" are adopted.

# SETUP

The analysis is done using the R programming language. All the code is incorporated in the writing, so it's clear which code chunk produces each piece of the output. Code parts are put in dark squares, to make it easy to separate them from the general text and the graphics. If you are not familiar with R, chances are that you can still follow along with the code, since the used syntax is relatively easy. For any questions/remarks you can also of course contact me [here](https://www.linkedin.com/in/erdi-myftaraga/).

## Libraries

The first step is to load few packages that will make life easier for us, since they facilitate a lot of stuff. We don't strictly need all these packages but few of them are used just for aesthetic purposes. I have also loaded some fonts from Google Fonts, mainly to make our plots and text look prettier.

```{r}
# libraries
library(tidyverse)
library(tidymodels)
library(stacks)
library(vroom)
library(readxl)
library(showtext)
library(slider)
library(bslib)
library(ggforce)
library(ggtext)
library(thematic)
library(kableExtra)
library(patchwork)
library(paletteer)

# fonts
font_add_google("Fira Code", "Fira Code")
showtext_auto()
```

Besides the code shown here, I'll also make use of few source files and functions which include some relatively long code to read and format AGS 4.0 files, to create plotting themes, etc. I excluded those from this post since it's already a long one.

## Functions and helpers

Here, some functions and properties we are going to use through the analysis.  

Colors to be used:

```{r}
# color preview functions
color_preview_dep <- 
  function() {
  htmltools::htmlDependency(
    name = "color_preview",
    version = "0.0.1",
    src = ".",
    all_files = FALSE,
    head = "
<style>.color-preview {
  display: inline-block;
  width: 1em;
  height: 1em;
  border-radius: 50%;
  margin: 0 0.33em;
  vertical-align: middle;
  transition: transform 100ms ease-in-out;
}

.color-preview:hover {
  cursor: pointer;
  transform: scale(2);
  transform-origin: 50% 50%;
}</style>"
  )
}

color_preview <- 
  function(color) {
  htmltools::tagList(
    htmltools::span(
      class = "color-preview",
      style = paste("background-color:", color),
      .noWS = "outside"
    ),
    htmltools::code(color, .noWS = "outside"),
    color_preview_dep()
  )
}
```

```{r}
# colors
col_r <- "#ef476f"
col_y <- "#ffd166"
col_g <- "#06d6a0"
col_b <- "#118ab2"

colors <- c(
  "#ef476f",
  "#ffd166",
  "#06d6a0",
  "#118ab2"
)

items_color <- 
  lapply(colors, function(color) {
  htmltools::tags$li(color_preview(color))
})
htmltools::tags$ul(items_color)
```

```{r}
# grays
gray_1 <- "#2F2F2FFF"
gray_2 <- "#474747FF"
gray_3 <- "#DFDFDFFF"
gray_4 <- "#F7F7F7FF"

grays <- c(
  "#2F2F2FFF",
  "#474747FF",
  "#DFDFDFFF",
  "#F7F7F7FF"
)

items_gray <- 
  lapply(grays, function(color) {
  htmltools::tags$li(color_preview(color))
})
htmltools::tags$ul(items_gray)
```

Function to create and style tables:

```{r}
# table function
func_tbl <- 
  function(dataframe){
  kable(dataframe, align = "r") %>%
  kable_styling(fixed_thead = T,
                html_font = ("JetBrains Mono"),
                font_size = 12,
                bootstrap_options = "condensed") %>%
  row_spec(0, background = gray_3) %>%
  scroll_box(height = "350px")
}
```

Theme and details for graphics:

```{r}
# update default theme
ggtheme <-
  theme_light() +
  theme(
    text = element_text(family = "Fira Code", size = 8.25, color = gray_1),
    plot.title = element_text(face = "bold", size = 8.25, hjust = 0.5),
    axis.title = element_text(face = "bold", size = 8.25),
    axis.text = element_text( size = 8.25, color = gray_2),
    panel.grid.major = element_line(color = gray_3, size = 0.25),
    panel.grid.minor = element_blank(),
    axis.ticks = element_line(color = gray_2, size = 0.25),
    axis.ticks.length = unit(.25, "cm"),
    panel.border = element_rect(color = gray_2, size = 0.25),
    strip.text = element_text(family = "Fira Code", size = 8.25, color = gray_1),
    legend.background = element_blank(),
    plot.background = element_blank(),
    panel.background = element_blank(),
    strip.background = element_blank(),
    legend.key = element_blank(),
    plot.margin = margin(t = 5, r = 10, b = 5, l = 10)
  )

# set theme
theme_set(ggtheme)

# details for annotations
annotation_fill <- gray_4
annotation_color <- gray_2
annotation_label.color <- NA
annotation_family <- "Fira Code"
annotation_size <- unit(2.75, "pt")
annotation_hjust <- 0.5
annotation_vjust <- 0.5
annotation_label.padding <- unit(2, "pt")
annotation_label.r <- unit(0, "pt")
annotation_fontface <- "bold"
```

# DATA

As mentioned, I am using data from the HKN offshore wind farm. The data is provided in few different formats (AGS 4.0, XLSX, etc), so we are going to read and analyze the relevant data separately and them merge/filter them, as we create the right setup for training our ML models.

## Locations

### Read locations data

It's important to initially have a general view of the site. For this, we would need the locations of all in-situ tests performed. This data can be extracted from the set of AGS 4.0 files, listed below.

```{r}
# list of source files
list_AGS <- list.files(pattern = ".ags")
func_tbl(as_tibble_col(list_AGS, column_name = "AGS file name"))
```

<br>

All seabed CPTU tests data are included in one file (the first row in the above table), while down-hole CPTUs are in separated files, one for each location. OSS data are not included here. I have previously downloaded the data locally, saved in my PC and read all AGS 4.0 files and converted them into one tabular CSV file.  

It has to be noted that, a distinction is made between `location` and `test point`. This is because several tests may have been run on essentially the same location, but considering that they don't exactly overlap and that they represent different tests, a distinction is made. Basically, the same location may include more than one test points. E.g. location HKN01 includes 4 test points: HKN02-BH-SA, HKN02-PCPT, HKN02-SCPT-A, and HKN02-TCPT.

Here we read the locations data and differentiate between various test types. In few locations we have duplicated tests of the same type, so (arbitrarily) we keep the deepest of these tests.

```{r}
# read location data
data_loc <-
  vroom("locations.csv") %>%
  select(c(LOCA_ID, LOCA_NATE, LOCA_NATN, LOCA_FDEP, LOCA_WDEP)) %>%
  mutate(across(LOCA_ID, as.factor)) %>%
  # pull out type of test and location
  mutate(
    TYPE = case_when(
      str_detect(LOCA_ID, "PCPT") ~ "PCPT",
      str_detect(LOCA_ID, "TCPT") ~ "TCPT",
      str_detect(LOCA_ID, "SCPT") ~ "SCPT",
      str_detect(LOCA_ID, "BH") ~ "BH"
    )
  ) %>%
  mutate(LOCATION = stringr::str_extract(LOCA_ID, "HKN..")) %>%
  # filter max depth for duplicated test points
  group_by(LOCA_ID) %>%
  filter(LOCA_FDEP == max(LOCA_FDEP)) %>%
  ungroup() %>%
  mutate(LOCA_NR = stringr::str_remove(LOCATION, "HKN")) %>%
  mutate(across(is.character, as.factor))

# table
func_tbl(data_loc)
```

<br>

Here:  

* `LOCA_ID`: ID of the test point;
* `LOCA_NATE` and `LOCA_NATN`: coordinates of the test point;
* `LOCA_FDEP`: final depth of the test;
* `LOCA_WDEP`: water depth;
* `TYPE`: type of the test;
  + `PCPT`: porepressure CPT;
  + `TCPT`: thermal CPTU;
  + `SCPT`: seismic CPTU;
  + `BH`: down-hole CPTU;
* `LOCATION`: location of the test point;
* `LOCA_NR`: number of the location.

We see that the same `LOCATION` doesn't have unique coordinates, but `LOCA_ID`s do. In this sense, a `LOCATION` can be seen as a cluster of different `LOCA_ID`s (test points).

### Explore locations

To understand the data better, we need to explore them a little bit. Some visualizations could be very helpful in this regard, considering we have a complex site here, with a lot of test points. Here, we have built a plan view of the site.

```{r fig.cap="_Plan view of the site._", fig.height=10}
ggplot(data_loc, aes(LOCA_NATE, LOCA_NATN)) +
  geom_point(aes(color = TYPE), size = 8, alpha = 1/4) +
  geom_richtext(
    data = data_loc, aes(label = LOCA_NR),
    color = annotation_color,
    family = annotation_family,
    size = annotation_size,
    fill = NA,
    label.color = annotation_label.color,
    label.padding = annotation_label.padding,
    label.r = annotation_label.r,
    hjust = annotation_hjust,
    vjust = annotation_vjust,
    fontface = annotation_fontface
  ) +
  facet_wrap(~TYPE) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  coord_cartesian(expand = c(0, 0)) +
  scale_color_manual(values = c(col_r, col_y, col_g, col_b)) +
  theme(legend.position = "none")
```

Here we have a lateral view from the EW direction, showing the maximum depth reached from each test `TYPE`.

```{r fig.cap="_Section view of the site from EW._", fig.height=10}
ggplot(data_loc) +
  geom_segment(aes(x = LOCA_NATE, y = -LOCA_WDEP, xend = LOCA_NATE, yend = -(LOCA_WDEP + LOCA_FDEP),
                   color = TYPE), size = 2, alpha = 1/4) +
  geom_hline(yintercept = 0, color = gray_2, size = 2, alpha = 1) + # sea level
  facet_wrap(~TYPE, ncol = 1) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  coord_cartesian(expand = c(0, 0)) +
  scale_color_manual(values = c(col_r, col_y, col_g, col_b)) +
  theme(legend.position = "none") +
  labs(y = "Depth")
```

There is basically one `PCTP` test for each location, while the number of `SCPT`s is smaller (normally), as the latest are performed only in a portion of all locations. Down-hole CPTUs, represented by `BH`s, complement the `PCPT` and `SCPT` tests, in depths where seabed CPTUs could not have been completed. `TCPT` tests are much shallower (up to 6m) and relevant to the design of subsea cables. We are not going to consider `TCPT`s in our analysis.

## CPTU data

### Read CPTU data

From the same set of AGS 4.0 files I have extracted the data recorded from the CPTU tests. Let's read these data and preview the first 100 rows.

```{r}
data_cpt <- 
  vroom("cpt.csv") %>%
  select(-c(SCPT_TEMP, FILE_FSET)) %>% # we don't need these
  mutate(across(is.character, as.factor))
data_cpt

# table
func_tbl(head(data_cpt, 100))
```

<br>

There are ~ 240k available data points here. We have the classic parameters we get from a CPTU test, with the addition of few derived parameters. More specifically:  

* `SCPG_TESN`: is the CPTU stroke or push;
* `SCPT_DPTH`: is the depth of result;
* `SCPT_RES`: is the cone resistance `qc`;
* `SCPT_FRES`: is the local unit side friction resistance `fs`;
* `SCPT_PWP2`: is the shoulder porewater pressure `u2`;
* `SCPT_FRR`: is the friction ratio `Rf`;
* `SCPT_QT`: is the corrected cone resistance `qt`;
* `SCPT_QNET`: is the net cone resistance `qn`;
* `SCPT_BQ`: is the pore pressure ratio `Bq`;
* `SCPT_CIC1`: is the soil behaviour type index `Ic`;
* `SCPT_CISB`: is the non-normalized soil behaviour type index `ISBT`;
* `SCPT_NQC1`: is the normalised cone resistance `Qt`;
* `SCPT_NRF1`: is the normalised friction ratio `Fr`;
* `SCPT_CUW1`: is the unit weight.

### Merge data

We merge the `data_loc` and `data_cpt` data, so everything is in one place and we can connect location to measured CPTU parameters. This makes it easier to explore and understand the data. We are also leaving out `TCPT` measurements.

```{r}
data_join <- left_join(data_loc, data_cpt, by = "LOCA_ID") %>% filter(TYPE != "TCPT")

# table
func_tbl(head(data_join, 100))
```

### Explore CPTUs {.tabset}

Here we plot some basic `CPTU` parameters, so we get some idea of the ground properties. Move through the tabs to see plots of `qc`, `fs` and `u2`, shown separated for each test type.

#### qc {-}

```{r fig.cap="_Plot of `qc` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_RES, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  labs(x = "qc (MPa)")
```

#### fs {-}

```{r fig.cap="_Plot of `fs` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_FRES, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  labs(x = "fs (kPa)")
```

#### u2 {-}

```{r fig.cap="_Plot of `u2` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_PWP2, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  labs(x = "u2 (kPa)")
```

### {-}

We need to filter out the _fake measurements_ from down-hole CPTU strokes. These are mostly the points on the left side of the BH plot. For simplicity, I am going to use the filtering implemented in the provided dataset. For this, we need to just remove the rows where `Qt` values are missing. If you'd like to implement your own criteria for filtering down-hole CPTs, I have created a small up for this purpose, which you may try [here](https://erdigg.shinyapps.io/downhole-app/). We are left with ~ 227k measurements.

```{r}
data_join <- data_join %>% filter(!is.na(SCPT_NQC1))
```

### Filtered plots {.tabset}

After removing the fake measurements and re-plotting, things look much better.

#### qc {-}

```{r fig.cap="_Plot of `qc` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_RES, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  labs(x = "qc (MPa)")
```

#### fs {-}

```{r fig.cap="_Plot of `fs` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_FRES, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  labs(x = "fs (kPa)")
```

#### u2 {-}

```{r fig.cap="_Plot of `u2` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_PWP2, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  labs(x = "u2 (kPa)")
```

### {-}

### Additional parameters {.tabset}

For reference, we also plot other derived `CPTU` parameters vs depth:

#### Fr {-}

```{r fig.cap="_Plot of `Fr` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_FRR, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(0, 10), ylim = c(80, 0)) +
  labs(x = "Fr (%)")
```

#### Ic {-}

```{r fig.cap="_Plot of `Ic` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_CIC1, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(1, 4), ylim = c(80, 0)) +
  labs(x = "Ic (-)")
```

#### Qn {-}

```{r fig.cap="_Plot of `Qn` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_NQC1, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(1, 1000), ylim = c(80, 0)) +
  scale_x_log10(breaks = c(1, 10, 100, 1000)) +
  labs(x = "Qtn (-)")
```

#### Unit weight {-}

```{r fig.cap="_Plot of `unit weight` vs depth._", fig.height=7}
ggplot(data_join, aes(SCPT_CUW1, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(16, 24), ylim = c(80, 0)) +
  labs(x = "Unit weight (kN/m3)")
```

### {-}

It looks like something is wrong with some of the normalised parameters and for the `unit weight`, as they show the same profile over and over for the `PCPT` and `SCPT` type of tests. I have no idea why this happens but probably, the data are overwritten somehow. ü§∑ We need to correct these parameters.  

The following code leaves out the "wrong" data and recalculates them. For the correlation of the unit weight (`UW`), we use the relationship given by Lengkeek et al., while for the other parameters (`U0`, `SV0`, `SV0P`, `Qtn`, `Fr`, `Ic`) we use the _classical_ formulas.

```{r}
data_corr <- 
  data_join %>%
  select(-c(SCPT_CIC1:SCPT_CUW1)) %>%
  mutate(
    # unit weight (Lengkeek et al.) (kN/m3)
    UW = 19 - 4.12 * (log10(5 / SCPT_QT) / log10(30 / SCPT_FRR)),
    # porewater pressure (kPa)
    U0 = 9.81 * SCPT_DPTH,
    # total in-situ vertical stress (kPa)
    SV0 = UW * SCPT_DPTH,
    # effective in-situ vertical stress (kPa)
    SV0P = SV0 - SCPT_DPTH * 9.81,
    # normalised cone resistance (-)
    QT1 = ((SCPT_QT - SV0 * 0.001) / (101 * 0.001)) * (101 / SV0P),
    # normalised friction ratio (%)
    Fr = (SCPT_FRES / (SCPT_QT - SV0 * 0.001) * 0.001) * 100,
    # soil behaviour type index Ic (-)
    IC1 = ((3.47 - log10(QT1))^2 + (log10(Fr) + 1.22)^2)^0.5,
    n1 = 0.381 * IC1 + 0.05 * (SV0P / 101) - 0.15,
    n = case_when(
      n1 > 1 ~ 1,
      n1 <= 1 ~ n1
    ),
    Qtn = ((SCPT_QT - SV0 * 0.001) / (101 * 0.001)) * (101 / SV0P)^n,
    Ic = ((3.47 - log10(Qtn))^2 + (log10(Fr) + 1.22)^2)^0.5
  ) %>%
  select(-c(SCPT_FRR, QT1, IC1, n1)) %>%
  mutate(round(across(UW:Ic), digits = 4))

# table
func_tbl(head(data_corr, 100))
```

### Plots of corrected parameters {.tabset}

Below some plots of these new and corrected parameters.

#### Fr {-}

```{r fig.cap="_Plot of `Fr` vs depth._", fig.height=7}
ggplot(data_corr, aes(Fr, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(0, 10), ylim = c(80, 0)) +
  labs(x = "Fr (%)")
```

#### Ic {-}

```{r fig.cap="_Plot of `Ic` vs depth._", fig.height=7}
ggplot(data_corr, aes(Ic, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(1, 4), ylim = c(80, 0)) +
  labs(x = "Ic (-)")
```

#### Qtn {-}

```{r fig.cap="_Plot of `Qtn` vs depth._", fig.height=7}
ggplot(data_corr, aes(Qtn, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(1, 1000), ylim = c(80, 0)) +
  scale_x_log10(breaks = c(1, 10, 100, 1000)) +
  labs(x = "Qtn (-)")
```

#### Unit weight {-}

```{r fig.cap="_Plot of `unit weight` vs depth._", fig.height=7}
ggplot(data_corr, aes(UW, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1) +
  scale_y_reverse() +
  facet_wrap(~TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(16, 24), ylim = c(80, 0)) +
  labs(x = "Unit weight (kN/m3)")
```

### {-}

## SCPT measurements

Measurements of share wave velocity `Vs` and shear modulus at small strain `Gmax` are obtained from seismic CPTU tests, performed both in seabed (`SCPT`) and down-hole CPTU tests (`BH`). `Vs` is derived from dual array geophones while `Gmax` is derived from `Vs`, using well-known theoretical relationships. SCPT measurements are provided in separated XLSX files, one file for each test point (see the following image).

<center>
![](seismic.PNG)
</center>

The list of these XLSX files is as follows:

```{r}
list_XLSX <- list.files(pattern = ".xlsx")

# table
func_tbl(as_tibble_col(list_XLSX, column_name = "XLSX file name"))
```

<br>

The following code reads the listed XLSX files, cleans the data a little bit and then merges them together into one data frame that includes all seismic measurements. Seismic measurements involve a vertical depth of 0.5m, i.e. the difference between `FROM` and `TO` is 0.5m. I created a new depth parameter, which coincides with the middle point between `FROM` and `TO`, and named it `SCPT_DPTH`. Further, I have extracted the `LOCA_ID` from the file names and inserted it in a column. These data manipulations, will allow us to easily merge seismic data to other data.

```{r}
data_seis <- 
  map(
  list_XLSX, ~ read_xlsx(.x, skip = 1) %>%
    mutate(LOCA_ID = basename(.x)) %>%
    dplyr::slice(-1)
) %>%
  bind_rows() %>%
  select(LOCA_ID, !LOCA_ID) %>%
  mutate(
    across(From:Gmax, as.double),
    across(is.double, round, 4)
  ) %>%
  # rename
  rename(
    FROM = From,
    TO = To,
    Vs = vs
  ) %>%
  # insert test point name
  mutate(
    LOCA_ID = stringr::str_remove(LOCA_ID, "HKN_20190221_FNLM_"),
    LOCA_ID = stringr::str_remove(LOCA_ID, "_XLSX_V04_F.xlsx"),
    LOCA_ID = stringr::str_remove(LOCA_ID, "HKN_20190306_FNLM_"),
    LOCA_ID = stringr::str_remove(LOCA_ID, "_XLSX_V05_F.xlsx")
  ) %>%
  mutate(across(LOCA_ID, as.factor)) %>%
  # new mean depth
  mutate(SCPT_DPTH = (FROM + TO) / 2) %>%
  mutate(across(SCPT_DPTH, round, 1)) %>%
  select(-(FROM:TO)) %>%
  mutate(LOCATION = stringr::str_extract(LOCA_ID, "HKN..")) %>%
  mutate(
    TYPE = case_when(
      str_detect(LOCA_ID, "PCPT") ~ "PCPT",
      str_detect(LOCA_ID, "TCPT") ~ "TCPT",
      str_detect(LOCA_ID, "SCPT") ~ "SCPT",
      str_detect(LOCA_ID, "BH") ~ "BH"
    )
  )

# table
func_tbl(data_seis)
```

<br>

In overall, we have a set of 570 measurements for `Vs` and `Gmax`. A very good data base to hopefully allow us to train some ML models. The following is a plot versus depth of `Vs` and `Gmax` and, as you could suspect, they are basically the same data üòÑ

```{r fig.cap="_Plot of measured `Vs` and `Gmax` vs depth._", fig.height=7}
plot_vs <- 
  ggplot(data_seis, aes(Vs, SCPT_DPTH)) +
  geom_point(alpha = 0.15, size = 5, color = gray_2) +
  scale_y_reverse() +
  coord_cartesian(expand = c(0, 0), xlim = c(0, NA), ylim = c(80, 0)) +
  labs(x = "Vs (m/s)")

plot_gmax <- 
  ggplot(data_seis, aes(Gmax, SCPT_DPTH)) +
  geom_point(alpha = 0.15, size = 5, color = gray_2) +
  scale_y_reverse() +
  coord_cartesian(expand = c(0, 0), xlim = c(0, NA), ylim = c(80, 0)) +
  labs(y = NULL) +
  labs(x = "Gmax (MPa)")

plot_vs | plot_gmax
```

## Putting it all together

Up to this point, we have 2 data sets:  

* `data_corr`: corrected CPTU data
* `data_seis`: seismic measurements data

Let's merge these data sets into one comprehensive data set,`data_all`.

```{r}
data_all <- data_corr %>% left_join(data_seis, by = c("LOCA_ID", "SCPT_DPTH", "LOCATION", "TYPE"))

# table
func_tbl(head(data_all, 100))
```

# MODELS

Here we start to build and train our models to predict `Gmax`. As previously mentioned, we aim to build a stacked ML model here, meaning that we are gong to blend several models together.

## Basic setup and data preparation

The models are obviously going to be trained using those data points where `Gmax` are present, so we need to extract this portion of the data from the overall dataset. But first, let's see if locations with `Gmax` measurements fully represent the whole site, i.e. if all the spectra of ground properties is covered.

```{r}
data_all <- 
  data_all %>%
  mutate(
    YES_NO = case_when(
      LOCA_ID %in% c(levels(data_seis$LOCA_ID)) ~ "with Gmax",
      TRUE ~ "without Gmax")
  )
```

```{r fig.cap="_Plot of `qc` vs depth and histograms of `qc` and `Ic` for locations with and without `Gmax` measurements._", fig.height=5, warning=FALSE}
plot_qc <- 
  ggplot(data_all, aes(SCPT_RES, SCPT_DPTH)) +
  geom_point(alpha = 0.05, size = 1, aes(color = YES_NO)) +
  scale_y_reverse() +
  facet_wrap(~YES_NO, ncol = 1) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  scale_color_manual(values = c(col_r, col_y)) +
  theme(legend.position = "none") +
  labs(x = "qc (MPa)")

hist_qc <- 
  ggplot(data_all, aes(SCPT_RES)) +
  geom_histogram(alpha = 0.5, aes(y =..density.., fill = YES_NO)) +
  facet_wrap(~YES_NO, scales = "free_y", ncol = 1) +
  coord_cartesian(expand = c(0, 0)) +
  scale_fill_manual(values = c(col_r, col_y)) +
  theme(legend.position = "none") +
  stat_function(data = data_all,
                aes(SCPT_RES),
                fun = dnorm,
                n = 200,
                args = list(mean = mean(data_all$SCPT_RES, na.rm = TRUE),
                            sd = sd(data_all$SCPT_RES, na.rm = TRUE)),
                colour = gray_2,
                size = 2,
                alpha = 0.5) +
  labs(x = "qc (MPa)")

hist_ic <- 
  ggplot(data_all, aes(Ic)) +
  geom_histogram(alpha = 0.5, aes(y =..density.., fill = YES_NO)) +
  facet_wrap(~YES_NO, scales = "free_y", ncol = 1) +
  coord_cartesian(expand = c(0, 0)) +
  scale_fill_manual(values = c(col_r, col_y)) +
  theme(legend.position = "none") +
  stat_function(data = data_all,
                aes(Ic),
                fun = dnorm,
                n = 200,
                args = list(mean = mean(data_all$Ic, na.rm = TRUE),
                            sd = sd(data_all$Ic, na.rm = TRUE)),
                colour = gray_2,
                size = 2,
                alpha = 0.5) +
  labs(x = "Ic (-)")

plot_qc | hist_qc | hist_ic
```

The gray line in the above plot shows the theoretical normal distribution of `qc` and `Ic` for all the data (with and without `Gmax`) and it can be used as a reference. It looks like the whole site is well covered with SCPT tests, except that maybe we have relatively less `SCPT`s in the deep layers (~ higher `qc` values).  

The next graph shows that there is actually a "gap" in the `SCPT` measurements after the depth of 20m.

```{r fig.cap="_Distribution of measurements along the depth for locations with and without Gmax measurements._", fig.height=7}
ggplot(data_all, aes(y = SCPT_DPTH)) +
  geom_histogram(alpha = 0.5, aes(x =..density.., fill = YES_NO)) +
  facet_wrap(~YES_NO, scales = "free_x", ncol = 2) +
  coord_cartesian(expand = c(0, 0), ylim = c(80, 0)) +
  scale_fill_manual(values = c(col_r, col_y)) +
  theme(legend.position = "none") +
  scale_y_reverse() +
  labs(x = "Density (quantity) of measurements")
```

Now we prepare the data we are going to use for our models. Let's first try to sync `Gmax` measurements to other CPTU measurements.  

As mentioned in the section on seismic measurements, `Vs` and `Gmax` data represent discrete measurements over 0.5m thickness of the test point, i.e. these data don't actually represent a point but some considerable soil volume. Since our aim is to find a relationship between `Gmax` and other parameters measured during a PCPT test, it makes sense to transform all parameters in such a way that each data point represents the properties of the ground for a thickness of 0.5m. In other words, for these parameters we have to somehow compute a moving average with a window of 0.5m (25 data points within one window).  

```{r}
# transform the data for syncing
data_corr_ml <- 
  data_corr %>%
  # correcting an unconsistency in depth data
  mutate(SCPT_DPTH = SCPT_DPTH * 100) %>%
  mutate(Depth = case_when(
    SCPT_DPTH %% 2 == 0 ~ round(SCPT_DPTH / 100, 1),
    SCPT_DPTH %% 2 != 0 ~ round((SCPT_DPTH + 1) / 100, 1)
  )) %>%
  mutate(SCPT_DPTH = SCPT_DPTH / 100) %>%
  # keeping relevant parameters
  select(-c(
    LOCA_NR,
    LOCA_NATE,
    LOCA_NATN,
    LOCA_FDEP,
    LOCA_WDEP,
    TYPE,
    LOCATION,
    SCPG_TESN,
    n,
    SV0,
    U0,
    Fr,
    SCPT_QT,
    SCPT_QNET,
    SCPT_PWP2
  )) %>%
  # moving average
    group_by(LOCA_ID) %>%
    mutate(
      SCPT_RES = round(slide_dbl(SCPT_RES, ~mean(.x), .before = 12, .after = 12), 4),
      SCPT_FRES = round(slide_dbl(SCPT_FRES, ~mean(.x), .before = 12, .after = 12), 4),
      SCPT_BQ = round(slide_dbl(SCPT_BQ, ~mean(.x), .before = 12, .after = 12), 4),
      UW = round(slide_dbl(UW, ~mean(.x), .before = 12, .after = 12), 4),
      SV0P = round(slide_dbl(SV0P, ~mean(.x), .before = 12, .after = 12), 4),
      Qtn = round(slide_dbl(Qtn, ~mean(.x), .before = 12, .after = 12), 4),
      Ic = round(slide_dbl(Ic, ~mean(.x), .before = 12, .after = 12), 4)
      ) %>%
    ungroup()

data_seis_ml <- 
  data_seis %>% rename(Depth = SCPT_DPTH)

# create ML models data
data_ml <- 
  data_seis_ml %>% 
  left_join(data_corr_ml, by = c("LOCA_ID", "Depth")) %>%
  distinct(LOCA_ID, Gmax, .keep_all = TRUE) %>%
  select(-c(LOCA_ID, Vs, SCPT_DPTH, LOCATION, TYPE)) %>%
  # few corrections
  fill(SCPT_FRES, UW, SV0P, Qtn, Ic, .direction = "down") %>%
  filter(Ic != "NaN")
  
func_tbl(data_ml)
```

So, 569 `Gmax` measurements. You probably noted that I removed some ID parameters or other parameters which I guessed don't have much predictive ability on the `Gmax`. A more rigorous procedure could have been followed here but this will do for now. `Vs` is also removed since it's basically a duplicate of `Gmax`.

## Data overview

So, the `data_ml` dataset (above table) is what we are going to use to train our models. Let's get a very short overview of these data.

```{r fig.cap="_Relationship between `Gmax` and other parameters._", fig.height=8.5, warning=FALSE}
ggplot(data_ml) +
  geom_point(aes(.panel_x, .panel_y), alpha = 1/5, size = 1) +
  facet_matrix(vars(Gmax:Ic)) +
  geom_smooth(aes(.panel_x, .panel_y),
              se = FALSE, span = 1, alpha = 0.25, size = 1.5, color = col_r) +
  coord_cartesian(expand = c(0, 0)) +
  theme(panel.grid.major = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

The previous plot shows the relationship of `Gmax` to the other predictive parameters. Here we can get an idea on the predicting power of these parameters. It looks like `Gmax` has a strong relationship to `Depth`, unit weight (`UW`) and effective vertical stress (`SV0P`). To my surprise, it looks like `Ic` doesn't have a clear relationship with `Gmax` ü§®.

## Data split

We need to split the data into:  

* `train data`: used to train the ML models (65 % of the data or 372 data points);
* `test data`: used to test the ML models (35 % of the data or 197 data points).

```{r}
# splitting
set.seed(1)
data_split <- initial_split(data_ml, prop = 0.7, strata = "Gmax")
data_train <- training(data_split)
data_test <- testing(data_split)
```

To avoid over-fitting and to allow some parameter tuning later, we also need to generate some resampling through cross validation.

```{r}
# resampling
set.seed(1)
folds <- rsample::vfold_cv(data_train, v = 10)
rec <- recipe(Gmax ~ ., data = data_train)
metric <- metric_set(rmse, rsq)

# control settings
ctrl_grid <- control_stack_grid()
ctrl_res <- control_stack_resamples()
```

What the `recipe` function says it is simply that we are trying to predict one outcome, which is `Gmax`, from 8 predictors (the rest of the parameters). It may look like these are a lot of predictors, which is kinda true, but these are dealt with using the `step_*` functions, which allow for some data preprocessing. For example, `step_corr` removes highly correlated predictors, to avoid overlapping. The `recipe` serves like the basis of model definitions, i.e. it will be a common feature for all models. After this, each model will develop it's own characteristics.  

We also specified also some metrics (`rmse`, `rsq`) which we are going to use to evaluate the performance/accuracy of the model. I particularly like to use the `rmse` (root mean square error) metric, since the result has the same unit as the predicted parameter and it's easier to get an idea of the level of accuracy and if it is acceptable to us.

## Models {.tabset}

Here we specify and train our models. I will obviously not go into great details here. Everything that I am going to use here comes from the [tidymodels](https://www.tidymodels.org/) metapackage, which is a collection of R packages for modeling and machine learning. It enables working with different ML models through a unified syntax (and much more). As you will notice later, we can switch from one ML model to another with minor changes in the code.  

Let's list all the models we are going to use:  

* Linear Regression (LR);
* K-Nearest Neighbor (KNN);
* Splines (SPL);
* Random Forest (RF);
* XGBoost (XGB).

Move through the tabs to see the code for specifying and fitting all models:

### LR {-}

Linear Regression:

```{r}
# create a model definition
lr_spec <-
  linear_reg() %>%
  set_engine("lm")

# extend the recipe
lr_rec <-
  rec %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors(), skip = TRUE)

# add both to a workflow
lr_wflow <- 
  workflow() %>%
  add_model(lr_spec) %>%
  add_recipe(lr_rec)

# fit to the 5-fold cv
set.seed(2021)
lr_res <- 
  fit_resamples(
    lr_wflow,
    resamples = folds,
    metrics = metric,
    control = ctrl_res
  )
```

### KNN {-}

K-Nearest Neighbor:

```{r}
# create a model definition
knn_spec <-
  nearest_neighbor(mode = "regression", 
                   neighbors = tune::tune("k")) %>%
  set_engine("kknn")

# extend the recipe
knn_rec <-
  rec %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors(), skip = TRUE) %>%
  step_meanimpute(all_numeric(), skip = TRUE) %>%
  step_normalize(all_numeric(), skip = TRUE)

# add both to a workflow
knn_wflow <- 
  workflow() %>% 
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

# tune k and fit to the 5-fold cv
set.seed(2021)
knn_res <- 
  tune_grid(
    knn_wflow,
    resamples = folds,
    metrics = metric,
    grid = 4,
    control = ctrl_grid
  )
```

### SPL {-}

Splines:

```{r}
# create a model definition
spl_spec <- 
  linear_reg() %>%
  set_engine("lm")

# extend the recipe
spl_rec <-
  rec %>%
  step_ns(SCPT_RES, UW, deg_free = tune::tune("length"))

# add both to a workflow
spl_wflow <- 
  workflow() %>% 
  add_model(spl_spec) %>%
  add_recipe(spl_rec)

# tune length and fit to the 5-fold cv
set.seed(2021)
spl_res <- 
  tune_grid(
    spl_wflow, 
    resamples = folds, 
    grid = 8,
    metrics = metric,
    control = ctrl_grid
  )
```

### RF {-}

Random Forest:

```{r}
# create a model definition
rf_spec <- 
  rand_forest(min_n = tune(), trees = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("randomForest")

rf_grid <- 
  grid_regular(parameters(rf_spec), levels = 4, filter = c(trees > 1))

# extend the recipe
rf_rec <-
  rec %>% 
  step_nzv(all_predictors()) %>% 
  step_corr(all_numeric(), -all_outcomes()) %>% 
  step_lincomb(all_numeric(), -all_outcomes()) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal())

# add both to a workflow
rf_wflow <- 
  workflow() %>% 
  add_model(rf_spec) %>%
  add_recipe(rf_rec)

# tune and fit to the 5-fold cv
set.seed(2021)
rf_res <- 
  tune_grid(
    rf_wflow,
    resamples = folds,
    grid = rf_grid,
    metrics = metric,
    control = ctrl_grid
  )
```

### XGB {-}

XGBoost:

```{r}
# create a model definition
xgb_spec <- 
  boost_tree(
    trees = 1000,
    tree_depth = tune(),
    min_n = tune(), 
    loss_reduction = tune(),
    sample_size = tune(), 
    mtry = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

xgb_grid <- 
  grid_latin_hypercube(
    tree_depth(),
    min_n(),
    loss_reduction(),
    sample_size = sample_prop(),
    finalize(mtry(), data_train),
    learn_rate(),
    size = 40
  )

# add recipe (formula) and merge into a workflow
xgb_wflow <- 
  workflow() %>%
  add_formula(Gmax ~ .) %>%
  add_model(xgb_spec)

# tune and fit to the 5-fold cv
set.seed(2021)
xgb_res <- 
  tune_grid(
    xgb_wflow,
    resamples = folds,
    grid = xgb_grid,
    metrics = metric,
    control = ctrl_grid
  )
```

### {-}

## Stacking and fitting

After defining and fitting all models, it's time to ensemble the results of these models in a data stack, named `gmax_st`.

```{r}
gmax_st <- 
  stacks() %>%
  add_candidates(lr_res) %>%
  add_candidates(knn_res) %>%
  add_candidates(spl_res) %>%
  add_candidates(rf_res) %>%
  add_candidates(xgb_res)
gmax_st
```

```{r}
# table
func_tbl(as_tibble(gmax_st))
```

<br>

Here, `gmax_st` is just a table where the 1st column shows the actual `Gmax` measurements while the other columns show the predictions of each _candidate members_. Note that due to tuning, some models are composed of several sub-models, with the aim of finding the optimal parameters that control the model. These sub-models are what we refer to as _candidate member_. Hence, what we have here is a stack with 5 model definitions and 65 candidate members. E.g. the XGBoost model has 40 candidate members (and that's why it gives 40 predictions of the `Gmax` value, corresponding to 40 columns in the stack), the RF model has 12 candidate members and so on.  

Now we evaluate how to combine (blend) and fit these predictions. Not all sub-models are necessarily included in the final ensemble.

```{r}
gmax_model_st <-
  gmax_st %>%
  blend_predictions() %>%
  fit_members()
```

## Predicting test data

Let's predict the test data (`data_test`) to see how the model performs.

```{r}
data_test_new <- 
  data_test %>%
  bind_cols(predict(gmax_model_st, .)) %>%
  relocate(.pred)

# table
func_tbl(data_test_new)
```

<br>

We can plot these predictions (`.pred` in the above table) and see how they compare to the actual measured data (`Gmax` in the table).  

* Scatter plot:

```{r fig.cap="_Plot of measured vs predicted `Gmax` values for the test data._", fig.height=7}
# data for annotations
data_ann <- 
  tibble(x = c(0, 0, 0, 25, 50, 265, 275),
         y = c(0, 25, 50, 0, 0, 275, 265),
         label = c("Pred. = Meas.", "+25 MPa", "+50 MPa", "-25 MPa", "-50 MPa",
                   "Overprediction", "Underprediction"))

# plot
ggplot(data_test_new) +
  aes(x = Gmax, 
      y = .pred) +
  geom_point(alpha = 0.15, size = 5, color = gray_2) + 
  coord_obs_pred() +
  geom_abline(size = 2, alpha = 1, color = col_r) +
  geom_abline(intercept = c(-25, 25), size = 2, alpha = 0.5, color = col_r) +
  geom_abline(intercept = c(-50, 50), size = 2, alpha = 0.2, color = col_r) +
  geom_richtext(
    data = data_ann,
    aes(x = x, y = y, label = label),
    color = annotation_color,
    family = annotation_family,
    size = unit(3, "pt"),
    fill = annotation_fill,
    label.color = annotation_label.color,
    label.padding = annotation_label.padding,
    label.r = annotation_label.r,
    hjust = 0,
    vjust = 0.5,
    fontface = annotation_fontface,
    angle = 45
  ) +
  labs(x = "Measured Gmax (MPa)", y = "Predicted Gmax (MPa)")
```

* Histogram:

It looks like our model struggles in accurately predicting the high values of `Gmax`. This is probably related to the decrease of SCPT measurements in deep layers (which are associated to high `Gmax` values).

```{r fig.cap="_Histogram of residuals for the test data._", fig.height=7}
# data for annotations
data_ann_hist <- 
  tibble(x = c(-2 * sd(data_test_new$.pred - data_test_new$Gmax),
               -1 * sd(data_test_new$.pred - data_test_new$Gmax),
               0,
               1 * sd(data_test_new$.pred - data_test_new$Gmax),
               2 * sd(data_test_new$.pred - data_test_new$Gmax)),
         y = 0.0175,
         label = c("-2SD", "-1SD", "Pred. = Meas.", "+1SD", "+2SD"))

data_ann_hist_sec <- 
  tibble(x = c(5, -5),
         y = c(0.015, 0.015),
         label = c("Overprediction", "Underprediction"))

# plot
ggplot(data_test_new, aes(.pred - Gmax)) +
  geom_histogram(binwidth = 7, aes(y =..density..), alpha = 0.15, fill = gray_2) +
  geom_vline(xintercept = 0,
             size = 2, alpha = 1, color = col_r) +
  geom_vline(data = data_test_new, aes(xintercept = sd(data_test_new$.pred - data_test_new$Gmax)),
             size = 2, alpha = 0.5, color = col_r) +
  geom_vline(data = data_test_new, aes(xintercept = -sd(data_test_new$.pred - data_test_new$Gmax)),
             size = 2, alpha = 0.5, color = col_r) +
  geom_vline(data = data_test_new, aes(xintercept = 2 * sd(data_test_new$.pred - data_test_new$Gmax)),
             size = 2, alpha = 0.2, color = col_r) +
  geom_vline(data = data_test_new, aes(xintercept = -2 * sd(data_test_new$.pred - data_test_new$Gmax)),
             size = 2, alpha = 0.2, color = col_r) +
  geom_richtext(
    data = data_ann_hist,
    aes(x = x, y = y, label = label),
    color = annotation_color,
    family = annotation_family,
    size = unit(3, "pt"),
    fill = annotation_fill,
    label.color = annotation_label.color,
    label.padding = annotation_label.padding,
    label.r = annotation_label.r,
    hjust = 0.5,
    vjust = 0.5,
    fontface = annotation_fontface,
    angle = 0
  ) +
  geom_richtext(
    data = data_ann_hist_sec,
    aes(x = x, y = y, label = label),
    color = annotation_color,
    family = annotation_family,
    size = unit(3, "pt"),
    fill = annotation_fill,
    label.color = annotation_label.color,
    label.padding = annotation_label.padding,
    label.r = annotation_label.r,
    hjust = 0.5,
    vjust = 0.5,
    fontface = annotation_fontface,
    angle = 90
  ) +
  labs(x = "'Predicted - Measured' Gmax (MPa)", y = NULL)
```

We can also assess the accuracy of the prediction in numerical terms, for the ensemble and for each candidate member. From the following table we see that the ensembled model outperforms (in terms of `rmse` and `rsq` values) all the other models, although some of them offer similar performance. We can also notice that some models are not appropriate for our case (or maybe I didn't define/tune them properly üòÖ).

```{r}
member_preds <- 
  data_test %>%
  select(Gmax) %>%
  bind_cols(predict(gmax_model_st, data_test, members = TRUE))

# rmse
accuracy_ml_rmse <- 
  map_dfr(member_preds, rmse, truth = Gmax, data = member_preds) %>%
  mutate(member = colnames(member_preds))

func_tbl(accuracy_ml_rmse)

# rsq
accuracy_ml_rsq <- 
  map_dfr(member_preds, rsq, truth = Gmax, data = member_preds) %>%
  mutate(member = colnames(member_preds))

func_tbl(accuracy_ml_rsq)
```

## Predicting the rest of the data

Now that our final ensemble model is fully defined, we can predict `Gmax` values from the rest of the data where CPTU measurements are available (basically, all of our site).

```{r}
# keep the rest of the data apart
data_corr_rest <- 
  data_corr %>%
  # correcting an unconsistency in depth data
  mutate(SCPT_DPTH = SCPT_DPTH * 100) %>%
  mutate(Depth = case_when(
    SCPT_DPTH %% 2 == 0 ~ round(SCPT_DPTH / 100, 1),
    SCPT_DPTH %% 2 != 0 ~ round((SCPT_DPTH + 1) / 100, 1)
  )) %>%
  mutate(SCPT_DPTH = SCPT_DPTH / 100)

data_seis_rest <- 
  data_seis %>% rename(Depth = SCPT_DPTH)

# create ML models data
data_rest <- 
  data_corr_rest %>% 
  left_join(data_seis_rest, by = c("LOCA_ID", "Depth", "TYPE", "LOCATION")) %>%
  filter(is.na(Gmax)) %>%
  # few corrections
  fill(SCPT_BQ, SCPT_RES, SCPT_FRES, UW, SV0P, Qtn, Ic, .direction = "down") %>%
  fill(SCPT_BQ, SCPT_RES, SCPT_FRES, UW, SV0P, Qtn, Ic, .direction = "up") %>%
  filter(Ic != "NaN", UW != "NaN")

# predict Gmax
data_rest_pred <- 
  data_rest %>%
  bind_cols(predict(gmax_model_st, .))
```

### Visualizing predictions {.tabset}

We need some graphics to see how the predictions look. The following graph shows the predicted `Gmax` values, separated by the type of the test. From this graphics we can create a general idea and it looks like things are good in the top 30 meters of the ground but they worsen deeper. 

#### Without measurements {-}

```{r fig.cap="_Plot of predicted `Gmax` values vs depth._", fig.height=8.5}
data_rest_pred %>%
  ggplot(aes(.pred, SCPT_DPTH)) +
  geom_point(aes(color = TYPE), alpha = 0.025) +
  scale_y_reverse() +
  facet_wrap(~ TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(0, 400), ylim = c(80, 0)) +
  #
  scale_color_manual(values = c(col_r, col_y, col_g, col_b)) +
  theme(legend.position = "none") +
  labs(x = "Predicted Gmax (MPa)")
```

#### With measurements {-}

```{r fig.cap="_Plot of predicted (color dots) and measured (gray dots) `Gmax` values vs depth._", fig.height=8.5}
data_rest_pred %>%
  ggplot(aes(.pred, SCPT_DPTH)) +
  geom_point(aes(color = TYPE), alpha = 0.025) +
  scale_y_reverse() +
  facet_wrap(~ TYPE) +
  coord_cartesian(expand = c(0, 0), xlim = c(0, 400), ylim = c(80, 0)) +
  geom_point(data = data_seis, aes(Gmax, SCPT_DPTH), alpha = 0.15, size = 5, color = gray_2) +
  scale_color_manual(values = c(col_r, col_y, col_g, col_b)) +
  theme(legend.position = "none") +
  labs(x = "Predicted Gmax (MPa)")
```

#### {-}

### Locations' predictions

A more detailed (and accurate) view can be achieved by plotting the results and predictions of _some_ locations. Note that we have `Gmax` measurements only in `BH` and `SCPT` test types (red and green dots).

```{r fig.cap="_Plot of predicted (color dots) and measured (gray dots) `Gmax` values vs depth (for each location)._", fig.height=17}
# filter data
data_seis_filt <- data_seis %>%
  filter(LOCATION %in% c("HKN06", "HKN10", "HKN25", "HKN27",
                         "HKN56", "HKN58", "HKN72", "HKN75"))

data_rest_pred %>%
  filter(LOCATION %in% c("HKN06", "HKN10", "HKN25", "HKN27",
                         "HKN56", "HKN58", "HKN72", "HKN75")) %>%
  ggplot(aes(.pred, SCPT_DPTH)) +
  geom_point(alpha = 0.15, size = 1, aes(colour = TYPE)) +
  scale_y_reverse() +
  scale_x_continuous(position = "top") +
  facet_wrap(~ LOCATION, ncol = 4) +
  coord_cartesian(expand = c(0, 0), xlim = c(0, 400), ylim = c(80, 0)) +
  geom_point(data = data_seis_filt, aes(Gmax, SCPT_DPTH), 
             alpha = 0.15, size = 5, color = gray_2) +
  scale_color_manual(values = c(col_r, col_y, col_g, col_b)) +
  theme(legend.position = "none") +
  labs(x = "Predicted Gmax (MPa)", y = NULL)
```

I am mostly happy with how, in some locations (e.g. `HKN25`, `HKN27`) the predictions follow the path/pattern of `Gmax` measurements. Sometimes, of course, there are considerable discrepancies between predictions and measurements (e.g. `HKN72` in the depth between ~32m and ~42m).  

There are few ways in which we can improve the accuracy of our predictions, but we can't really go into much detail here... this post is already too long üò´.

***

That's it! Thanks for reading üëã

***